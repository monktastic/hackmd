# Bell's Inequalities for Dummies

## Einstein, Podolsky, and Rosen

In 1935, Einstein and two colleagues published the [now-famous EPR paper](https://cds.cern.ch/record/405662/files/PhysRev.47.777.pdf) arguing that quantum mechanics was incomplete. The original argument was [abstruse and convoluted](https://plato.stanford.edu/entries/qt-epr/), but it boils down to this: when two entangled particles are measured, their results are strongly correlated. This correlation must arise either from shared hidden properties determined at creation, or from communication between particles during measurement. Since faster-than-light communication is impossible (per special relativity), the particles must have hidden properties determining their behavior. But quantum mechanics doesn't describe these properties, instead treating measurement outcomes as fundamentally random. Therefore, quantum mechanics must be incomplete, and some more complete theory must exist that describes these hidden properties.

For decades, nobody could think of a satisfying way to experimentally prove whether this argument was correct. Did particles actually have well-defined properties prior to measurement, or was measurement itself somehow involved in bringing those properties into existence? It seemed logically impossible to distinguish these possibilities. Then, in 1964, physicist John Stewart Bell made a brilliant discovery: certain types of measurements could definitively rule out Einstein's view. If particles did have preset properties as Einstein believed, their correlations would have to follow certain mathematical rules — and these rules, now known as _Bell's inequalities_, could be experimentally tested. Remarkably, quantum mechanics predicted these rules would be violated...

## The simplified version

Imagine you have a large collection of items, each defined by three binary properties called A, B, and C. Each property can be either 0 or 1, meaning there are eight possible combinations in total. For example, one item might have A=0, B=0, C=1, while another might have A=1, B=1, C=0. Now suppose you count the total number of items where A=0 and B=0, regardless of the value of C. Let’s denote this quantity as `N(0, 0, x)`, where x means “don’t care.” A simple inequality that can be proven is:

`N(0, 0, x) ≤ N(x, 0, 0) + N(0, x, 1)`

In words: the number of particles where properties A and B are both zero is less than or equal to the number where B and C are zero _plus_ the number where A is 0 and C is 1. Why does this hold? It follows from two basic observations:

- `N(0, 0, 0) ≤ N(x, 0, 0)`, because the latter includes both `N(0, 0, 0)` and `N(1, 0, 0)`.
- `N(0, 0, 1) ≤ N(0, x, 1)`, because the latter includes both `N(0, 0, 1)` and `N(0, 1, 1)`.

Adding these two gives us the original inequality.

Now suppose that instead of measuring the properties of all particles in your collection at once, you draw them one at a time and measure all three properties on each, tallying whether they fall into each of the three "buckets" (terms) in our inequality. Some will fall into none, some will fall in one, and some will fall in two. But at all times, the inequality will hold — because, again, any time an item falls in the left bucket, it must _also_ fall in one of the two right buckets.

Now let's constrain it further, and say that for each particle, we can only measure _two_ of its properties, ignoring the third. What happens? Well now, for each item we can only increase the tally for at most one of the buckets, corresponding to the two properties we measured. Therefore, the inequality will not necessarily hold at all times: for example, we might increment `N(0, 0, x)` without having enough information to increment the others. Yet if we repeat this process enough times, it ought to hold in the long run, as long as we sample each of the three pairs of properties equally. Why? Because then we're effectively just estimating the true tallies for each of the buckets amongst our population.

Is there a way to "defeat" our test — that is, to make the inequality fail in the long run? Well, one way that could happen is if our sampling is biased in a way that correlates with our choices of measurement. Suppose a demon is messing with our sampling. If it knows that we're going to measure A and B, it could ensure that A=0 and B=0, thus growing the left count. If it knows we will measure B and C or A and C, it ensures that we get samples that do _not_ increment the buckets on the right.

Okay, but are there any "weaker" ways to defeat the inequality? What if, after we measure the first chosen property, the particle _randomly_ changes the values of its other two properties. Would this work? No: it would be just as if we had drawn a different item to begin with. To actually break the inequality, the change in the other two properties would have to _depend_ on the result of the first measurement.

What does this tell us? It tells us that _if_ these particles have well-defined values for the three properties in advance of being measured — even if they are allowed to "squirm around" during the process — then they cannot escape this inequality, _unless_ that "squirming" depends on either what _will be measured_ or what _has already been measured_. There's no way for the particles to use predetermined rules to defeat the inequality.

## The quantum version

The quantum version of this experiment requires some modifications. Before measuring the sampled particle, we will first split it into two identical copies. Whatever "predetermined rules" one has, so does the other, so that they always behave in exactly the same way. These represent our entangled particles. Now, we move them very far apart from each other, and instead of measuring two properties on a single particle, we measure one property on one of the particles and another property on the other. We still tally them as though they were measurements on a single item (representing their common state). In practice, the chosen properties are usually the spins (or polarizations) of the particles along various axes (which, according to QM, are _non-commuting observables_). The actual inequalities (called _Bell's Inequalities_) are not exactly the ones given above, but the details aren't important here.

If our assumptions are correct, and the measurements are indeed being determined by some shared pre-existing state, then they cannot defeat our inequality. The only way to defeat it is if _after_ measuring one, the other one "squirms" in a way that _depends_ on the result of the first — but that's impossible, because it would require faster-than-light communication. And yet when we do this experiment, the inequality _is_ broken! This violation was first experimentally demonstrated by Alain Aspect and others in the 1980s. In 2022, Aspect and two others received the Nobel Prize for their groundbreaking work.

This means that Einstein was wrong: there is no possible local "hidden state" (or what we now call "hidden variables") that can account for these results. Therefore, the particles _cannot_ have definite states prior to measurement, and QM is not (necessarily) incomplete. Does this mean that locality is violated — that information is propagating faster than the speed of light? Not quite. It turns out that there is no way to communicate information using this process. Whatever is happening simply doesn't fit into our classical intuitions. It should be noted that there are theoretically other ways to wriggle out of this conclusion. For example, a principle called _superdeterminism_ says that something like a grand conspiracy really is true: your choice of detector settings is not free, but will always be magically correlated with the state of the particles somehow. Needless to say, this is not a popular hypothesis.

## Implications

Imagine you are God, inventing the universe. Suppose you wanted it to be the case that the future is not fixed, but always indeterminate — and moreover, you want the inhabitants of your universe to be able to mathematically prove this (perhaps to fend off ennui). How would you do such a thing? It wouldn't be sufficient to give it classical physics with some randomness sprinkled in (e.g., a coin toss now and then to alter its behavior). That would provide the indeterminacy, but not the ability to prove it: the inhabitants would always wonder if there was actually a pattern that they just hadn't detected yet. It turns out that QM is probably the _simplest_ possible set of rules that allow for it, granting some other basic assumptions.

This is an aspect of the whole thing that I find under-appreciated, but fascinating to ponder.